{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "___\n",
    "# NN Training Study\n",
    "## __By:__ Jos√© Luis Almendarez Gonz√°lez"
   ],
   "id": "dfcfb0ca678ec5d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Abstract\n",
    "\n",
    "### Methodology\n",
    "\n",
    "### Results\n",
    "\n",
    "### Analysis\n",
    "\n",
    "### Limitations\n",
    "\n",
    "### References\n",
    "___"
   ],
   "id": "101b0e996d4b2af9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Python libraries *",
   "id": "c47047b66f668caf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T15:28:01.396875Z",
     "start_time": "2025-10-26T15:27:58.909681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os, random\n",
    "from torch.utils.data import ConcatDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import lib\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import ipywidgets"
   ],
   "id": "8d201983abb265d2",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " \\* Many of these libraries are properly referenced in the \"lib.py\", an imported document used for presentation.",
   "id": "c5f9a8673e534e4f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data Import, Split, Augmentation & Preparation\n",
    "___"
   ],
   "id": "4fa2c8f09141b44a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_data = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_data = MNIST(root='./data', train=False, download=True, transform=transform)"
   ],
   "id": "8690e873ec1eef62"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(len(train_data))\n",
    "print(len(test_data))"
   ],
   "id": "3f420828a75d958b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "base_dir = './data/MNIST/use/original'\n",
    "\n",
    "for split in ['train', 'test']:\n",
    "    path = os.path.join(base_dir, split)\n",
    "    os.makedirs(path, exist_ok=True)"
   ],
   "id": "5c9acf3d16c77c6e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def save_subset(dataset, split_dir, n_per_class=10, train_ratio=0.8):\n",
    "    counts = {i:0 for i in range(10)}\n",
    "\n",
    "    for img, label in dataset:\n",
    "        if counts[label] < n_per_class:\n",
    "            counts[label] += 1\n",
    "\n",
    "            # Decidir si va a train o test\n",
    "            target_split = 'train' if counts[label] <= n_per_class * train_ratio else 'test'\n",
    "            save_dir = os.path.join(split_dir, target_split, str(label))  # subcarpeta por clase\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "            # Guardar imagen con el label en el nombre de archivo\n",
    "            img_path = os.path.join(save_dir, f'{label}_{counts[label]}.jpg')\n",
    "            img_pil = transforms.ToPILImage()(img)\n",
    "            img_pil.save(img_path)"
   ],
   "id": "7a7a686ee799dac0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "save_subset(train_data, base_dir, n_per_class=60, train_ratio=0.8)\n",
    "save_subset(test_data, base_dir, n_per_class=60, train_ratio=0.8)"
   ],
   "id": "5a24ec3752b89939"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "base_dir = './data/MNIST/use/original'\n",
    "\n",
    "for split in ['train', 'test']:\n",
    "    split_path = os.path.join(base_dir, split)\n",
    "    num_images = sum(\n",
    "        len([f for f in os.listdir(os.path.join(split_path, cls)) if f.endswith('.jpg')])\n",
    "        for cls in os.listdir(split_path) if os.path.isdir(os.path.join(split_path, cls))\n",
    "    )\n",
    "    print(num_images)"
   ],
   "id": "7b8b363460be7d98"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "augment_transform = transforms.Compose([\n",
    "    transforms.RandomRotation(45),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomResizedCrop(28, scale=(0.8, 1.0)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
    "])"
   ],
   "id": "29353738f0382f48"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def generate_augmented(input_dir, output_dir, target_count):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    classes = [cls for cls in os.listdir(input_dir) if os.path.isdir(os.path.join(input_dir, cls))]\n",
    "\n",
    "    for cls in classes:\n",
    "        input_class_dir = os.path.join(input_dir, cls)\n",
    "        output_class_dir = os.path.join(output_dir, cls)\n",
    "        os.makedirs(output_class_dir, exist_ok=True)\n",
    "\n",
    "        images = [f for f in os.listdir(input_class_dir) if f.endswith('.jpg')]\n",
    "        current_count = 0  # solo aumentadas\n",
    "        counter = 0\n",
    "\n",
    "        while current_count < target_count:\n",
    "            img_name = random.choice(images)\n",
    "            img = Image.open(os.path.join(input_class_dir, img_name))\n",
    "            aug_img = augment_transform(img)\n",
    "            counter += 1\n",
    "            new_name = f\"{img_name.split('.')[0]}_aug{counter}.jpg\"\n",
    "            aug_img.save(os.path.join(output_class_dir, new_name))\n",
    "            current_count += 1"
   ],
   "id": "2fe00a1a7eafe63b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "original_train = './data/MNIST/use/original/train'\n",
    "original_test = './data/MNIST/use/original/test'\n",
    "\n",
    "augmented_train = './data/MNIST/use/augmented/train'\n",
    "augmented_test = './data/MNIST/use/augmented/test'\n",
    "\n",
    "generate_augmented(original_train, augmented_train, target_count=432)\n",
    "generate_augmented(original_test, augmented_test, target_count=108)\n"
   ],
   "id": "27440d35e9c0d6f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "base_dir = './data/MNIST/use/augmented'\n",
    "\n",
    "for split in ['train', 'test']:\n",
    "    split_path = os.path.join(base_dir, split)\n",
    "    num_images = sum(\n",
    "        len([f for f in os.listdir(os.path.join(split_path, cls)) if f.endswith('.jpg')])\n",
    "        for cls in os.listdir(split_path) if os.path.isdir(os.path.join(split_path, cls))\n",
    "    )\n",
    "    print(num_images)"
   ],
   "id": "c6a25f0f0296f6b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T15:27:12.846404Z",
     "start_time": "2025-10-26T15:27:12.807253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = ConcatDataset([\n",
    "    ImageFolder('./data/MNIST/use/original/train'),\n",
    "    ImageFolder('./data/MNIST/use/augmented/train')\n",
    "])\n",
    "\n",
    "test_dataset = ConcatDataset([\n",
    "    ImageFolder('./data/MNIST/use/original/test'),\n",
    "    ImageFolder('./data/MNIST/use/augmented/test'),\n",
    "])"
   ],
   "id": "bf3af72fad87ebae",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ],
   "id": "17848fdacea29c9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_images(base_dir, titulo):\n",
    "\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    clases = sorted([cls for cls in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, cls))], key=int)\n",
    "\n",
    "    for idx, cls in enumerate(clases):\n",
    "        class_dir = os.path.join(base_dir, cls)\n",
    "        img_name = sorted([f for f in os.listdir(class_dir) if f.endswith('.jpg')])[0]\n",
    "        img_path = os.path.join(class_dir, img_name)\n",
    "\n",
    "        img = Image.open(img_path)\n",
    "        img_tensor = transforms.ToTensor()(img)\n",
    "\n",
    "        axes[idx].imshow(img_tensor.squeeze(), cmap='gray')\n",
    "        axes[idx].set_title(f\"Clase {cls}\")\n",
    "        axes[idx].axis('off')\n",
    "\n",
    "    fig.suptitle(titulo, fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "id": "18f1447895e64e28"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Original train/test\n",
    "plot_images('./data/MNIST/use/original/train', \"Original Train\")\n",
    "plot_images('./data/MNIST/use/original/test', \"Original Test\")\n",
    "\n",
    "# Augmented train/test\n",
    "plot_images('./data/MNIST/use/augmented/train', \"Augmented Train\")\n",
    "plot_images('./data/MNIST/use/augmented/test', \"Augmented Test\")"
   ],
   "id": "6c5339fe203351ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(len(train_dataset)+len(test_dataset))",
   "id": "bcefe4a40a5ced5c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T15:28:15.802692Z",
     "start_time": "2025-10-26T15:28:15.799916Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=1),   # Primero grayscale\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])   # 1 canal\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])"
   ],
   "id": "ca8d885fbb2190a4",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T15:28:17.175979Z",
     "start_time": "2025-10-26T15:28:17.152901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset = ConcatDataset([\n",
    "    ImageFolder('./data/MNIST/use/original/train', transform=train_transform),\n",
    "    ImageFolder('./data/MNIST/use/augmented/train', transform=train_transform)\n",
    "])\n",
    "\n",
    "test_dataset = ConcatDataset([\n",
    "    ImageFolder('./data/MNIST/use/original/test', transform=test_transform),\n",
    "    ImageFolder('./data/MNIST/use/augmented/test', transform=test_transform)\n",
    "])"
   ],
   "id": "bf9e064662e85547",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T15:28:18.881272Z",
     "start_time": "2025-10-26T15:28:18.876966Z"
    }
   },
   "cell_type": "code",
   "source": "train_dataset, test_dataset",
   "id": "a24020d03d4ab429",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataset.ConcatDataset at 0x10cbeb950>,\n",
       " <torch.utils.data.dataset.ConcatDataset at 0x10cc26720>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Build & Evaluate Model(s)\n",
    "___"
   ],
   "id": "e7fe2590bbdc2255"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-26T21:00:46.622954Z",
     "start_time": "2025-10-26T21:00:46.492185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, models\n",
    "from pathlib import Path\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------\n",
    "# Configuraci√≥n base\n",
    "# -------------------------\n",
    "results_dir = Path(\"results\")\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Modelo factory + init\n",
    "# -------------------------\n",
    "def get_model(model_name=\"alexnet\", num_classes=10, weight_init=\"kaiming\", dropout_rate=0.0, input_size=(1,224,224)):\n",
    "    if model_name.lower() == \"alexnet\":\n",
    "        model = models.alexnet(weights=None)\n",
    "        model.features[0] = nn.Conv2d(1, 64, kernel_size=11, stride=4, padding=2)\n",
    "        in_features = model.classifier[-1].in_features\n",
    "        layers = list(model.classifier.children())[:-1]\n",
    "        if dropout_rate > 0:\n",
    "            layers.insert(-1, nn.Dropout(dropout_rate))\n",
    "        layers.append(nn.Linear(in_features, num_classes))\n",
    "        model.classifier = nn.Sequential(*layers)\n",
    "\n",
    "    elif model_name.lower() == \"vgg\":\n",
    "        model = models.vgg11(weights=None)\n",
    "        model.features[0] = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "        in_features = model.classifier[-1].in_features\n",
    "        layers = list(model.classifier.children())[:-1]\n",
    "        if dropout_rate > 0:\n",
    "            layers.insert(-1, nn.Dropout(dropout_rate))\n",
    "        layers.append(nn.Linear(in_features, num_classes))\n",
    "        model.classifier = nn.Sequential(*layers)\n",
    "\n",
    "    elif model_name.lower() == \"customcnn\":\n",
    "        class CustomCNN(nn.Module):\n",
    "            def __init__(self, num_classes=10, dropout_rate=0.0, input_size=(1,224,224)):\n",
    "                super().__init__()\n",
    "                self.features = nn.Sequential(\n",
    "                    nn.Conv2d(1, 32, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "                    nn.Conv2d(32, 64, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2),\n",
    "                    nn.Conv2d(64, 128, 3, 1, 1), nn.ReLU(), nn.MaxPool2d(2)\n",
    "                )\n",
    "                with torch.no_grad():\n",
    "                    x = torch.zeros(1, *input_size)\n",
    "                    x = self.features(x)\n",
    "                    flatten_size = x.view(1, -1).shape[1]\n",
    "                self.classifier = nn.Sequential(\n",
    "                    nn.Flatten(),\n",
    "                    nn.Linear(flatten_size, 256), nn.ReLU(),\n",
    "                    nn.Dropout(dropout_rate) if dropout_rate > 0 else nn.Identity(),\n",
    "                    nn.Linear(256, 128), nn.ReLU(),\n",
    "                    nn.Dropout(dropout_rate) if dropout_rate > 0 else nn.Identity(),\n",
    "                    nn.Linear(128, num_classes)\n",
    "                )\n",
    "            def forward(self, x):\n",
    "                return self.classifier(self.features(x))\n",
    "        model = CustomCNN(num_classes=num_classes, dropout_rate=dropout_rate, input_size=input_size)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Modelo desconocido: {model_name}\")\n",
    "\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            if weight_init.lower() == \"kaiming\":\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\")\n",
    "            elif weight_init.lower() == \"xavier\":\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "            elif weight_init.lower() == \"orthogonal\":\n",
    "                nn.init.orthogonal_(m.weight)\n",
    "            if hasattr(m, \"bias\") and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    return model\n",
    "\n",
    "# -------------------------\n",
    "# Helper: scheduler + warmup\n",
    "# -------------------------\n",
    "def set_constant_with_warmup(optimizer, base_lr, epoch, warmup_epochs=5):\n",
    "    if epoch <= warmup_epochs and warmup_epochs > 0:\n",
    "        lr = base_lr * epoch / float(warmup_epochs)\n",
    "    else:\n",
    "        lr = base_lr\n",
    "    for g in optimizer.param_groups:\n",
    "        g['lr'] = lr\n",
    "    return lr\n",
    "\n",
    "\n",
    "def lr_range_test(model, train_loader, optimizer_class, lr_start=1e-6, lr_end=1, num_iters=100, device=\"cpu\"):\n",
    "    \"\"\"Realiza un LR Range Test y devuelve las p√©rdidas por LR.\"\"\"\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optimizer_class(model.parameters(), lr=lr_start)\n",
    "\n",
    "    lrs, losses = [], []\n",
    "    mult = (lr_end / lr_start) ** (1/num_iters)\n",
    "    lr = lr_start\n",
    "\n",
    "    iterator = iter(train_loader)\n",
    "    for i in range(num_iters):\n",
    "        try:\n",
    "            images, labels = next(iterator)\n",
    "        except StopIteration:\n",
    "            iterator = iter(train_loader)\n",
    "            images, labels = next(iterator)\n",
    "\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        lrs.append(lr)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        lr *= mult\n",
    "        for g in optimizer.param_groups:\n",
    "            g['lr'] = lr\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(lrs, losses)\n",
    "    plt.xscale(\"log\")\n",
    "    plt.xlabel(\"Learning Rate (log scale)\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"LR Range Test\")\n",
    "    plt.show()\n",
    "\n",
    "    return lrs, losses\n",
    "\n",
    "# -------------------------\n",
    "# Main training function\n",
    "# -------------------------\n",
    "def train_model(model_name,\n",
    "                train_dataset, test_dataset,\n",
    "                batch_size=128,\n",
    "                optimizer_name=\"SGD\",\n",
    "                nesterov=False,\n",
    "                lr=0.01,\n",
    "                weight_decay=5e-4,\n",
    "                epochs=60,\n",
    "                dropout_rate=0.0,\n",
    "                weight_init=\"kaiming\",\n",
    "                label_smoothing=0.0,\n",
    "                lr_schedule=\"constant+warmup\",\n",
    "                use_augment=False,\n",
    "                protocol=\"Fixed Epochs\",\n",
    "                wall_clock_budget=None,\n",
    "                seed=42,\n",
    "                device=None,\n",
    "                num_workers=2,\n",
    "                pin_memory=True):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    if device is None:\n",
    "        if torch.cuda.is_available():\n",
    "            device = torch.device(\"cuda\"); print(\"Usando dispositivo: CUDA\")\n",
    "        elif torch.backends.mps.is_available():\n",
    "            device = torch.device(\"mps\"); print(\"Usando dispositivo: MPS\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\"); print(\"Usando dispositivo: CPU\")\n",
    "\n",
    "    if use_augment:\n",
    "        extra = transforms.Compose([\n",
    "            transforms.RandomRotation(30),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "            transforms.RandomPerspective(distortion_scale=0.2, p=0.5)\n",
    "        ])\n",
    "        if hasattr(train_dataset, \"transform\") and train_dataset.transform is not None:\n",
    "            train_dataset.transform = transforms.Compose([extra, train_dataset.transform])\n",
    "        else:\n",
    "            train_dataset.transform = extra\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                              num_workers=num_workers, pin_memory=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                             num_workers=num_workers, pin_memory=False)\n",
    "\n",
    "    model = get_model(model_name=model_name, num_classes=10, weight_init=weight_init,\n",
    "                      dropout_rate=dropout_rate, input_size=(1,224,224))\n",
    "    model.to(device)\n",
    "\n",
    "    opt_lower = optimizer_name.lower()\n",
    "    if opt_lower == \"sgd\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay, nesterov=False)\n",
    "    elif opt_lower == \"sgd+nesterov\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay, nesterov=True)\n",
    "    elif opt_lower == \"adam\":\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    elif opt_lower == \"adamw\":\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    else:\n",
    "        raise ValueError(\"Optimizador desconocido\")\n",
    "\n",
    "    scheduler = None\n",
    "    if lr_schedule == \"step\":\n",
    "        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30,50], gamma=0.1)\n",
    "    elif lr_schedule == \"cosine\":\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    elif lr_schedule == \"one_cycle\":\n",
    "        scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(train_loader), epochs=epochs)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n",
    "\n",
    "    logs = []\n",
    "    train_losses, train_accs, test_accs = [], [], []\n",
    "    epoch_times, epoch_lrs = [], []\n",
    "\n",
    "    start_time = time.time()\n",
    "    epoch = 1\n",
    "\n",
    "    best_val_acc = -1.0\n",
    "    best_model_state = None\n",
    "\n",
    "    while True:\n",
    "        if protocol == \"Fixed Epochs\" and epoch > epochs:\n",
    "            break\n",
    "        if protocol == \"Fixed Wall-Clock Time\" and wall_clock_budget is not None and (time.time() - start_time) > wall_clock_budget:\n",
    "            break\n",
    "\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0\n",
    "        running_total = 0\n",
    "        t0 = time.time()\n",
    "\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\", disable=True):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, preds = outputs.max(1)\n",
    "            running_correct += preds.eq(labels).sum().item()\n",
    "            running_total += labels.size(0)\n",
    "\n",
    "        epoch_time = time.time() - t0\n",
    "        if lr_schedule == \"constant+warmup\":\n",
    "            cur_lr = set_constant_with_warmup(optimizer, base_lr=lr, epoch=epoch, warmup_epochs=5)\n",
    "        else:\n",
    "            cur_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        if scheduler is not None and lr_schedule != \"one_cycle\":\n",
    "            scheduler.step()\n",
    "\n",
    "        epoch_loss = running_loss / running_total if running_total > 0 else float('nan')\n",
    "        epoch_acc = running_correct / running_total if running_total > 0 else 0.0\n",
    "\n",
    "        # --- evaluation ---\n",
    "        model.eval()\n",
    "        test_correct = 0\n",
    "        test_total = 0\n",
    "        test_loss_accum = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss_accum += loss.item() * images.size(0)\n",
    "                _, preds = outputs.max(1)\n",
    "                test_correct += preds.eq(labels).sum().item()\n",
    "                test_total += labels.size(0)\n",
    "        test_loss = test_loss_accum / test_total if test_total > 0 else float('nan')\n",
    "        test_acc = test_correct / test_total if test_total > 0 else 0.0\n",
    "\n",
    "        # --- checkpoint: actualizar mejor modelo ---\n",
    "        if test_acc > best_val_acc:\n",
    "            best_val_acc = test_acc\n",
    "            best_model_state = model.state_dict()\n",
    "\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accs.append(epoch_acc)\n",
    "        test_accs.append(test_acc)\n",
    "        epoch_times.append(epoch_time)\n",
    "        epoch_lrs.append(cur_lr)\n",
    "\n",
    "        logs.append({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": float(epoch_loss),\n",
    "            \"train_acc\": float(epoch_acc),\n",
    "            \"test_loss\": float(test_loss),\n",
    "            \"test_acc\": float(test_acc),\n",
    "            \"epoch_time_s\": float(epoch_time),\n",
    "            \"lr\": float(cur_lr)\n",
    "        })\n",
    "\n",
    "        print(f\"Epoch {epoch}/{epochs} | loss {epoch_loss:.4f} | train_acc {epoch_acc:.4f} | test_acc {test_acc:.4f} | lr {cur_lr:.5g} | time {epoch_time:.1f}s\")\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        model.eval()\n",
    "        test_correct_final = 0\n",
    "        test_total_final = 0\n",
    "        test_loss_accum_final = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss_accum_final += loss.item() * images.size(0)\n",
    "                _, preds = outputs.max(1)\n",
    "                test_correct_final += preds.eq(labels).sum().item()\n",
    "                test_total_final += labels.size(0)\n",
    "    final_test_loss = test_loss_accum_final / test_total_final\n",
    "    final_test_acc = test_correct_final / test_total_final\n",
    "    print(f\"\\nüß™ Test final ‚Äî Loss: {final_test_loss:.4f} | Acc: {final_test_acc:.4f}\")\n",
    "\n",
    "    # --- plots ---\n",
    "    fig, axes = plt.subplots(1,2,figsize=(12,4))\n",
    "    axes[0].plot([l[\"epoch\"] for l in logs], [l[\"train_loss\"] for l in logs], label=\"train_loss\")\n",
    "    axes[0].plot([l[\"epoch\"] for l in logs], [l[\"test_loss\"] for l in logs], label=\"test_loss\")\n",
    "    axes[0].set_title(\"Loss por √©poca\"); axes[0].legend()\n",
    "    axes[1].plot([l[\"epoch\"] for l in logs], [l[\"train_acc\"] for l in logs], label=\"train_acc\")\n",
    "    axes[1].plot([l[\"epoch\"] for l in logs], [l[\"test_acc\"] for l in logs], label=\"test_acc\")\n",
    "    axes[1].set_title(\"Accuracy por √©poca\"); axes[1].legend()\n",
    "    plt.show()\n",
    "\n",
    "    model_dir = results_dir / model_name.lower()\n",
    "    model_dir.mkdir(exist_ok=True)\n",
    "    exp_idx = len(list(model_dir.glob(\"exp_*\"))) + 1\n",
    "    exp_path = model_dir / f\"exp_{exp_idx}\"\n",
    "    exp_path.mkdir(exist_ok=True)\n",
    "\n",
    "    metrics_df = pd.DataFrame(logs)\n",
    "    metrics_df.to_json(exp_path / \"metrics.json\", index=False)\n",
    "\n",
    "    metadata = {\n",
    "        \"model_name\": model_name,\n",
    "        \"optimizer\": optimizer_name,\n",
    "        \"nesterov_flag\": True if optimizer_name.lower()==\"sgd+nesterov\" else False,\n",
    "        \"lr\": lr,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"weight_init\": weight_init,\n",
    "        \"dropout_rate\": dropout_rate,\n",
    "        \"label_smoothing\": label_smoothing,\n",
    "        \"lr_schedule\": lr_schedule,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"use_augment\": use_augment,\n",
    "        \"protocol\": protocol,\n",
    "        \"wall_clock_budget_s\": wall_clock_budget,\n",
    "        \"seed\": seed,\n",
    "        \"device\": str(device),\n",
    "        \"num_epochs_run\": len(logs),\n",
    "        \"total_train_time_s\": sum(epoch_times),\n",
    "        \"epoch_times_s\": epoch_times,\n",
    "        \"epoch_lrs\": epoch_lrs,\n",
    "        \"best_val_acc\": float(best_val_acc),\n",
    "        \"tqdm_disabled_display\": True\n",
    "    }\n",
    "    with open(exp_path / \"metadata.json\", \"w\") as f:\n",
    "        json.dump(metadata, f, indent=4)\n",
    "\n",
    "    # --- Save button ---\n",
    "    def _save_model(btn):\n",
    "        torch.save(best_model_state, exp_path / \"model.pth\")\n",
    "        metadata[\"model_file\"] = str((exp_path / \"model.pth\").resolve())\n",
    "        with open(exp_path / \"metadata.json\", \"w\") as f:\n",
    "            json.dump(metadata, f, indent=4)\n",
    "        print(f\"\\n‚úì Mejor modelo guardado en {exp_path / 'model.pth'} con test_acc = {best_val_acc:.4f}\")\n",
    "\n",
    "    save_btn = widgets.Button(description=\"üíæ Guardar modelo y metadata\", button_style=\"info\")\n",
    "    save_btn.on_click(_save_model)\n",
    "    display(save_btn)\n",
    "\n",
    "    print(f\"\\n‚úÖ M√©tricas (metrics.json) y metadata.json creados en: {exp_path}\")\n",
    "    return exp_path\n",
    "\n",
    "# -------------------------\n",
    "# Widget UI (igual que tu c√≥digo original)\n",
    "# -------------------------\n",
    "model_widget = widgets.Dropdown(options=[\"alexnet\",\"vgg\",\"customcnn\"], value=\"alexnet\", description=\"Modelo\")\n",
    "optimizer_widget = widgets.Dropdown(options=[\"SGD\",\"SGD+Nesterov\",\"Adam\",\"AdamW\"], value=\"SGD\", description=\"Optimizer\")\n",
    "nesterov_checkbox = widgets.Checkbox(value=True, description=\"Nesterov (when applicable)\")\n",
    "nesterov_checkbox.layout.display = \"none\"\n",
    "\n",
    "def on_opt_change(change):\n",
    "    if change[\"new\"].lower() == \"sgd+nesterov\":\n",
    "        nesterov_checkbox.layout.display = \"block\"\n",
    "    else:\n",
    "        nesterov_checkbox.layout.display = \"none\"\n",
    "optimizer_widget.observe(on_opt_change, names=\"value\")\n",
    "on_opt_change({\"new\": optimizer_widget.value})\n",
    "\n",
    "batch_widget = widgets.Dropdown(options=[32,128,512], value=128, description=\"Batch size\")\n",
    "lr_widget = widgets.FloatLogSlider(value=0.01, base=10, min=-4, max=-1, step=0.1, description=\"LR\")\n",
    "weight_decay_widget = widgets.FloatSlider(value=5e-4, min=0.0, max=0.01, step=1e-4, description=\"Weight Decay\")\n",
    "dropout_widget = widgets.Checkbox(value=False, description=\"Dropout\")\n",
    "dropout_rate_widget = widgets.FloatSlider(value=0.5, min=0.0, max=0.9, step=0.05, description=\"Dropout Rate\")\n",
    "weight_init_widget = widgets.Dropdown(options=[\"kaiming\",\"xavier\",\"orthogonal\"], value=\"kaiming\", description=\"Weight Init\")\n",
    "lr_schedule_widget = widgets.Dropdown(options=[\"constant+warmup\",\"step\",\"cosine\",\"one_cycle\"], value=\"constant+warmup\", description=\"LR Schedule\")\n",
    "label_smooth_widget = widgets.FloatSlider(value=0.0, min=0.0, max=0.2, step=0.01, description=\"Label Smoothing\")\n",
    "augment_widget = widgets.Checkbox(value=False, description=\"Augment Extra\")\n",
    "\n",
    "protocol_widget = widgets.Dropdown(options=[\"Fixed Epochs\",\"Fixed Wall-Clock Time\"], value=\"Fixed Epochs\", description=\"Protocol\")\n",
    "epochs_box = widgets.BoundedIntText(value=60, min=1, max=100, description=\"Epochs\")\n",
    "time_box = widgets.BoundedIntText(value=90, min=1, max=120, description=\"Time (min)\")\n",
    "time_box.layout.display = \"none\"\n",
    "seed_box = widgets.BoundedIntText(value=42, min=0, max=9999, description=\"Seed\")\n",
    "\n",
    "def on_protocol_change(change):\n",
    "    if change[\"new\"] == \"Fixed Epochs\":\n",
    "        epochs_box.layout.display = \"block\"\n",
    "        time_box.layout.display = \"none\"\n",
    "    else:\n",
    "        epochs_box.layout.display = \"none\"\n",
    "        time_box.layout.display = \"block\"\n",
    "protocol_widget.observe(on_protocol_change, names=\"value\")\n",
    "on_protocol_change({\"new\": protocol_widget.value})\n",
    "\n",
    "train_button = widgets.Button(description=\"üöÄ Entrenar\", button_style=\"success\")\n",
    "lrtest_button = widgets.Button(description=\"üîé LR Range Test\", button_style=\"warning\")\n",
    "output_widget = widgets.Output(layout={'border': '1px solid black'})\n",
    "\n",
    "def on_lrtest_clicked(btn):\n",
    "    with output_widget:\n",
    "        clear_output(wait=True)\n",
    "        print(\"üîé Ejecutando LR Range Test...\\n\")\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "        model = get_model(model_name=model_widget.value, num_classes=10,\n",
    "                          weight_init=weight_init_widget.value,\n",
    "                          dropout_rate=dropout_rate_widget.value if dropout_widget.value else 0.0)\n",
    "        model.to(device)\n",
    "        optimizer_class = {\n",
    "            \"SGD\": lambda params, lr: optim.SGD(params, lr=lr, momentum=0.9),\n",
    "            \"SGD+Nesterov\": lambda params, lr: optim.SGD(params, lr=lr, momentum=0.9, nesterov=True),\n",
    "            \"Adam\": lambda params, lr: optim.Adam(params, lr=lr),\n",
    "            \"AdamW\": lambda params, lr: optim.AdamW(params, lr=lr)\n",
    "        }[optimizer_widget.value]\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=int(batch_widget.value), shuffle=True)\n",
    "        lr_range_test(model, train_loader, optimizer_class, device=device)\n",
    "\n",
    "def on_train_clicked(btn):\n",
    "    with output_widget:\n",
    "        clear_output(wait=True)\n",
    "        print(\"üöÄ Iniciando entrenamiento...\\n\")\n",
    "        wall_budget = None\n",
    "        if protocol_widget.value == \"Fixed Wall-Clock Time\":\n",
    "            wall_budget = time_box.value * 60\n",
    "\n",
    "        try:\n",
    "            exp_path = train_model(\n",
    "                model_name = model_widget.value,\n",
    "                train_dataset = train_dataset,  # debe existir\n",
    "                test_dataset  = test_dataset,   # debe existir\n",
    "                batch_size = int(batch_widget.value),\n",
    "                optimizer_name = optimizer_widget.value,\n",
    "                nesterov = bool(nesterov_checkbox.value),\n",
    "                lr = float(lr_widget.value),\n",
    "                weight_decay = float(weight_decay_widget.value),\n",
    "                epochs = int(epochs_box.value),\n",
    "                dropout_rate = float(dropout_rate_widget.value) if dropout_widget.value else 0.0,\n",
    "                weight_init = weight_init_widget.value,\n",
    "                label_smoothing = float(label_smooth_widget.value),\n",
    "                lr_schedule = lr_schedule_widget.value,\n",
    "                use_augment = bool(augment_widget.value),\n",
    "                protocol = protocol_widget.value,\n",
    "                wall_clock_budget = wall_budget,\n",
    "                seed = int(seed_box.value),\n",
    "                device = None,\n",
    "                num_workers = 2,\n",
    "                pin_memory = True\n",
    "            )\n",
    "            print(f\"\\nüèÅ Experimento completado. Archivos en: {exp_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error durante entrenamiento: {e}\")\n",
    "\n",
    "train_button.on_click(on_train_clicked)\n",
    "lrtest_button.on_click(on_lrtest_clicked)\n",
    "\n",
    "ui = widgets.VBox([\n",
    "    widgets.HBox([model_widget, optimizer_widget, nesterov_checkbox]),\n",
    "    widgets.HBox([batch_widget, lr_widget, weight_decay_widget]),\n",
    "    widgets.HBox([dropout_widget, dropout_rate_widget, weight_init_widget]),\n",
    "    widgets.HBox([lr_schedule_widget, label_smooth_widget, augment_widget]),\n",
    "    widgets.HBox([protocol_widget, epochs_box, time_box, seed_box]),\n",
    "    widgets.HBox([train_button, lrtest_button]),\n",
    "    output_widget\n",
    "])\n",
    "display(ui)\n"
   ],
   "id": "9530b76807899b89",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Modelo', options=('alexnet', 'vgg', 'customcnn'), value='a‚Ä¶"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2399dd98cedf4c538ff5c66d0b403a10"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "___",
   "id": "3d20d010b995233d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
